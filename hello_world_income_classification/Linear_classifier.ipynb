{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdDIzmtn2A65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import six.moves.urllib.request as request\n",
        "import tensorflow as tf\n",
        "\n",
        "from distutils.version import StrictVersion\n",
        "\n",
        "# Check that we have correct TensorFlow version installed\n",
        "tf_version = tf.__version__\n",
        "print(\"TensorFlow version: {}\".format(tf_version))\n",
        "assert StrictVersion(\"1.4\") <= StrictVersion(tf_version), \"TensorFlow r1.4 or later is needed\"\n",
        "\n",
        "# Windows users: You only need to change PATH, rest is platform independent\n",
        "PATH = \"/tmp/tf_dataset_and_estimator_apis\"\n",
        "\n",
        "# Fetch and store Training and Test dataset files\n",
        "\n",
        "PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "PATH_test = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        "\n",
        "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
        "FILE_TRAIN = PATH_DATASET + os.sep + \"income_data_train.csv\"\n",
        "FILE_TEST = PATH_DATASET + os.sep + \"income_data_test.csv\"\n",
        "URL_TRAIN = https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
        "URL_TEST = URL_TRAIN\n",
        "\n",
        "def download_dataset(url, file):\n",
        "    if not os.path.exists(PATH_DATASET):\n",
        "        os.makedirs(PATH_DATASET)\n",
        "    if not os.path.exists(file):\n",
        "        data = request.urlopen(url).read()\n",
        "        with open(file, \"wb\") as f:\n",
        "            f.write(data)\n",
        "            f.close()\n",
        "            \n",
        "download_dataset(URL_TRAIN, FILE_TRAIN)\n",
        "download_dataset(URL_TEST, FILE_TEST)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# The CSV features in our training & test data\n",
        "feature_names = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\t\t\t\t\n",
        "                \"dis\", \"tax\", \"ptratio\",\"lstat\",\"medv\"]\n",
        "\n",
        "# Create an input function reading a file using the Dataset API\n",
        "# Then provide t\n",
        "\n",
        "# Create an input function reading a file using the Dataset API\n",
        "# Then provide the results to the Estimator API\n",
        "\n",
        "\n",
        "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
        "    def decode_csv(line):\n",
        "        parsed_line = tf.decode_csv(line, [[0.0], [0.0], [0.0], [0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]])\n",
        "        # Everything but last elements are the features\n",
        "        feature_dict = dict(zip(feature_names, parsed_line))\n",
        "        label = feature_dict.pop('medv')\n",
        "        return feature_dict, label\n",
        "\n",
        "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
        "               .skip(1)  # Skip header row\n",
        "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
        "    if perform_shuffle:\n",
        "        # Randomizes input using a window of 256 elements (read into memory)\n",
        "        dataset = dataset.shuffle(buffer_size=256)\n",
        "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
        "    dataset = dataset.batch(32)  # Batch size to use\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    batch_features, batch_labels = iterator.get_next()\n",
        "    return batch_features, batch_labels\n",
        "\n",
        "next_batch = my_input_fn(FILE_TRAIN, True)  # Will return 32 random elements\n",
        "\n",
        "'''\n",
        "with tf.Session() as sess:\n",
        "    first_batch = sess.run(next_batch)\n",
        "print(first_batch)\n",
        "'''\n",
        "\n",
        "# Create the feature_columns, which specifies the input to our model\n",
        "# All our input features are numeric, so use numeric_column for each one\n",
        "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names if k != \"medv\" ]\n",
        "\n",
        "print(\"columns: \", len(feature_columns))\n",
        "# Create a deep neural network regression classifier\n",
        "# Use the DNNClassifier pre-made estimator\n",
        "model = tf.estimator.LinearRegressor(feature_columns=feature_columns, model_dir=PATH)# Path to where checkpoints etc are stored\n",
        "\n",
        "# Train our model, use the previously defined function my_input_fn\n",
        "# Input to training is a file with training example\n",
        "# Stop training after 8 iterations of train data (epochs)\n",
        "model.train(steps=1000,\n",
        "    input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))\n",
        "\n",
        "'''\n",
        "# Evaluate our model using the examples contained in FILE_TEST\n",
        "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
        "evaluate_result = model.evaluate(\n",
        "    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
        "print(\"Evaluation results\")\n",
        "for key in results:   \n",
        "  print(\"   {}, was: {}\".format(key, results[key]))\t\n",
        "\n",
        "\n",
        "# Predict the type of some Iris flowers.\n",
        "# Let's predict the examples in FILE_TEST, repeat only once.\n",
        "predict_results = model.predict(\n",
        "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
        "print(\"Predictions on test file\")\n",
        "for prediction in predict_results:\n",
        "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
        "    # is Iris Sentosa, Vericolor, Virginica, respectively.\n",
        "    print(prediction[\"class_ids\"][0])\n",
        "\n",
        "# Let create a dataset for prediction\n",
        "# We've taken the first 3 examples in FILE_TEST\n",
        "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
        "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
        "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
        "\n",
        "\n",
        "def new_input_fn():\n",
        "    def decode(x):\n",
        "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
        "        return dict(zip(feature_names, x))  # To build a dict of them\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
        "    dataset = dataset.map(decode)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    next_feature_batch = iterator.get_next()\n",
        "    return next_feature_batch, None  # In prediction, we have no labels\n",
        "\n",
        "# Predict all our prediction_input\n",
        "predict_results = classifier.predict(input_fn=new_input_fn)\n",
        "\n",
        "# Print results\n",
        "print(\"Predictions:\")\n",
        "for idx, prediction in enumerate(predict_results):\n",
        "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
        "    if type == 0:\n",
        "        print(\"  I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
        "    elif type == 1:\n",
        "        print(\"  I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
        "    else:\n",
        "        print(\"  I think: {}, is Iris Virginica\".format(prediction_input[idx]))\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}